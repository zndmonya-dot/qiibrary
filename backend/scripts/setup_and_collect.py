"""
BookTuber ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
2. YouTube APIã‹ã‚‰å®Ÿéš›ã®å‹•ç”»ã‚’æ¤œç´¢
3. å‹•ç”»èª¬æ˜ã‹ã‚‰Amazonãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
4. Amazon APIã§æ›¸ç±æƒ…å ±ã‚’å–å¾—
5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
"""

import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from datetime import datetime, timedelta
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import NullPool

from app.config import settings
from app.database import Base
from app.models.book import Book, BookDailyStat
from app.models.youtube_video import YouTubeVideo, BookMention
from app.models.search_keyword import SearchKeyword
from app.services.youtube_service import YouTubeService
from app.services.amazon_link_extractor import AmazonLinkExtractor
from app.services.amazon_service import get_amazon_service
from app.config import get_high_priority_keywords, should_exclude_video, SEARCH_CONFIG


class BookTuberSetup:
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.engine = create_engine(
            settings.DATABASE_URL,
            poolclass=NullPool,
            echo=False
        )
        self.SessionLocal = sessionmaker(bind=self.engine)
        self.youtube_service = YouTubeService(settings.YOUTUBE_API_KEY)
        self.link_extractor = AmazonLinkExtractor()
        
    def create_tables(self):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆä¸­...")
        try:
            Base.metadata.create_all(bind=self.engine)
            print("âœ… ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†\n")
        except Exception as e:
            print(f"âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def clear_existing_data(self):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢"""
        print("ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ä¸­...")
        db = self.SessionLocal()
        try:
            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼ˆPostgreSQLï¼‰
            db.execute(text("SET CONSTRAINTS ALL DEFERRED;"))
            
            # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
            db.query(BookDailyStat).delete()
            db.query(BookMention).delete()
            db.query(YouTubeVideo).delete()
            db.query(SearchKeyword).delete()
            db.query(Book).delete()
            
            db.commit()
            print("âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Œäº†\n")
        except Exception as e:
            db.rollback()
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        finally:
            db.close()
    
    def search_videos_for_keyword(self, keyword: str, locale: str, max_results: int = 20):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å‹•ç”»ã‚’æ¤œç´¢"""
        try:
            videos = self.youtube_service.search_videos(
                query=keyword,
                locale=locale,
                max_results=max_results
            )
            print(f"  âœ“ '{keyword}' ã§ {len(videos)} ä»¶ã®å‹•ç”»ã‚’ç™ºè¦‹")
            return videos
        except Exception as e:
            print(f"  âœ— '{keyword}' ã®æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def extract_amazon_links_from_video(self, video: dict, locale: str):
        """å‹•ç”»ã‹ã‚‰Amazonãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        description = video.get('description', '')
        amazon_info = self.link_extractor.extract_amazon_info(description)
        
        # ãƒ­ã‚±ãƒ¼ãƒ«ã«åˆã£ãŸASINã®ã¿ã‚’æŠ½å‡º
        asins = []
        for info in amazon_info:
            asin = info.get('asin')
            marketplace = info.get('marketplace', 'jp')
            
            # ãƒ­ã‚±ãƒ¼ãƒ«ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if locale == 'ja' and marketplace == 'jp':
                asins.append(asin)
            elif locale == 'en' and marketplace == 'com':
                asins.append(asin)
            elif marketplace == 'unknown':
                # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ—ãƒ¬ã‚¤ã‚¹ãŒä¸æ˜ãªå ´åˆã¯ç¾åœ¨ã®ãƒ­ã‚±ãƒ¼ãƒ«ã¨ä»®å®š
                asins.append(asin)
        
        return asins
    
    def get_book_info_from_asin(self, asin: str, locale: str):
        """ASINã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’å–å¾—"""
        amazon_service = get_amazon_service(locale)
        
        try:
            # Amazon APIã‹ã‚‰å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆåŒæœŸç‰ˆï¼‰
            book_info = amazon_service.get_book_info_sync(asin, locale)
            if book_info:
                print(f"  âœ“ Amazon API: '{book_info.get('title', asin)}' ã®æƒ…å ±ã‚’å–å¾—")
                return book_info
        except Exception as e:
            print(f"  âš  Amazon API ã‚¨ãƒ©ãƒ¼ (ASIN: {asin}): {e}")
        
        # APIãŒå¤±æ•—ã—ãŸå ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        domain = 'amazon.co.jp' if locale == 'ja' else 'amazon.com'
        print(f"  â„¹ ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨: {asin}")
        return {
            'asin': asin,
            'title': f"Book {asin}",
            'author': None,
            'publisher': None,
            'publication_date': None,
            'price': None,
            'sale_price': None,
            'discount_rate': None,
            'rating': None,
            'review_count': None,
            'image_url': f"https://m.media-amazon.com/images/I/{asin}.jpg",
            'description': None,
            'amazon_url': f"https://www.{domain}/dp/{asin}",
            'affiliate_url': f"https://www.{domain}/dp/{asin}?tag={amazon_service.associate_tag}",
            'locale': locale,
        }
    
    def collect_data_for_locale(self, locale: str, max_keywords: int = 5):
        """ç‰¹å®šãƒ­ã‚±ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ ãƒ­ã‚±ãƒ¼ãƒ«: {locale.upper()}")
        print(f"{'='*60}\n")
        
        db = self.SessionLocal()
        try:
            # é«˜å„ªå…ˆåº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
            keywords = get_high_priority_keywords(locale)[:max_keywords]
            print(f"ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}\n")
            
            # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å‹•ç”»ã‚’æ¤œç´¢
            all_videos = []
            for keyword in keywords:
                videos = self.search_videos_for_keyword(keyword, locale, max_results=10)
                all_videos.extend(videos)
            
            print(f"\nğŸ“º åˆè¨ˆ {len(all_videos)} ä»¶ã®å‹•ç”»ã‚’å–å¾—")
            
            # é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_videos = []
            for video in all_videos:
                if not should_exclude_video(video['title'], video.get('description', '')):
                    filtered_videos.append(video)
            
            print(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_videos)} ä»¶\n")
            
            # å‹•ç”»ã‹ã‚‰Amazonãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            book_mentions = {}  # {asin: {'book': book_data, 'videos': [video_data]}}
            
            for video in filtered_videos:
                # Amazonãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                amazon_links = self.extract_amazon_links_from_video(video, locale)
                
                if amazon_links:
                    print(f"ğŸ“š [{video['title'][:40]}...] ã‹ã‚‰ {len(amazon_links)} å†Šç™ºè¦‹")
                    
                    for asin in amazon_links:
                        if asin not in book_mentions:
                            # æ›¸ç±æƒ…å ±ã‚’å–å¾—
                            book_info = self.get_book_info_from_asin(asin, locale)
                            book_mentions[asin] = {
                                'book': book_info,
                                'videos': []
                            }
                        
                        book_mentions[asin]['videos'].append(video)
            
            print(f"\nğŸ“– åˆè¨ˆ {len(book_mentions)} å†Šã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ›¸ç±ã‚’ç™ºè¦‹\n")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            print("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ä¸­...\n")
            
            book_count = 0
            video_count = 0
            mention_count = 0
            
            for asin, data in book_mentions.items():
                book_info = data['book']
                videos = data['videos']
                
                # æ›¸ç±ã‚’ä¿å­˜
                existing_book = db.query(Book).filter_by(asin=asin).first()
                if not existing_book:
                    book = Book(
                        asin=book_info['asin'],
                        title=book_info['title'],
                        author=book_info.get('author'),
                        publisher=book_info.get('publisher'),
                        publication_date=book_info.get('publication_date'),
                        price=book_info.get('price'),
                        sale_price=book_info.get('sale_price'),
                        discount_rate=book_info.get('discount_rate'),
                        rating=book_info.get('rating'),
                        review_count=book_info.get('review_count'),
                        image_url=book_info.get('image_url'),
                        description=book_info.get('description'),
                        amazon_url=book_info['amazon_url'],
                        affiliate_url=book_info['affiliate_url'],
                        locale=locale,
                        total_views=0,
                        total_mentions=0,
                    )
                    db.add(book)
                    db.flush()
                    book_count += 1
                else:
                    book = existing_book
                
                # å‹•ç”»ã‚’ä¿å­˜
                for video_data in videos:
                    video_id = video_data['video_id']
                    
                    existing_video = db.query(YouTubeVideo).filter_by(video_id=video_id).first()
                    if not existing_video:
                        # published_at ãŒæ–‡å­—åˆ—ã®å ´åˆã¯ datetime ã«å¤‰æ›
                        published_at_raw = video_data.get('published_at', datetime.now())
                        if isinstance(published_at_raw, str):
                            # ISOå½¢å¼ã®æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
                            published_at = datetime.fromisoformat(published_at_raw.replace('Z', '+00:00'))
                        else:
                            published_at = published_at_raw
                        
                        video = YouTubeVideo(
                            video_id=video_id,
                            title=video_data['title'],
                            description=video_data.get('description', ''),
                            channel_id=video_data.get('channel_id', 'unknown'),
                            channel_name=video_data.get('channel_name', 'Unknown'),
                            thumbnail_url=video_data.get('thumbnail_url'),
                            video_url=f"https://www.youtube.com/watch?v={video_id}",
                            view_count=video_data.get('view_count', 0),
                            like_count=video_data.get('like_count', 0),
                            published_at=published_at,
                            locale=locale,
                        )
                        db.add(video)
                        db.flush()
                        video_count += 1
                    else:
                        video = existing_video
                    
                    # é–¢é€£ä»˜ã‘ã‚’ä¿å­˜
                    existing_mention = db.query(BookMention).filter_by(
                        book_id=book.id,
                        video_id=video.id
                    ).first()
                    
                    if not existing_mention:
                        mention = BookMention(
                            book_id=book.id,
                            video_id=video.id,
                            mentioned_at=video.published_at,
                        )
                        db.add(mention)
                        mention_count += 1
                    
                    # æ›¸ç±ã®çµ±è¨ˆã‚’æ›´æ–°
                    book.total_views += video.view_count
                    book.total_mentions += 1
                    
                    # datetimeã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’çµ±ä¸€ã—ã¦æ¯”è¼ƒ
                    video_pub_naive = video.published_at.replace(tzinfo=None) if video.published_at.tzinfo else video.published_at
                    book_mention_naive = book.latest_mention_at.replace(tzinfo=None) if (book.latest_mention_at and book.latest_mention_at.tzinfo) else book.latest_mention_at
                    
                    if not book_mention_naive or video_pub_naive > book_mention_naive:
                        book.latest_mention_at = video.published_at
            
            db.commit()
            
            print(f"âœ… {book_count} å†Šã®æ›¸ç±ã‚’ä¿å­˜")
            print(f"âœ… {video_count} ä»¶ã®å‹•ç”»ã‚’ä¿å­˜")
            print(f"âœ… {mention_count} ä»¶ã®é–¢é€£ä»˜ã‘ã‚’ä¿å­˜")
            
        except Exception as e:
            db.rollback()
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼ ({locale}): {e}")
            raise
        finally:
            db.close()
    
    def generate_statistics(self):
        """çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        print("\n" + "="*60)
        print("ğŸ“Š çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        print("="*60 + "\n")
        
        db = self.SessionLocal()
        try:
            # éå»30æ—¥åˆ†ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            books = db.query(Book).all()
            today = datetime.now().date()
            
            stats_count = 0
            
            for book in books:
                for days_ago in range(30):
                    date = today - timedelta(days=days_ago)
                    
                    # ãã®æ—¥ã®çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    daily_views = book.total_views // 30  # å‡ç­‰åˆ†æ•£ï¼ˆä»®ï¼‰
                    daily_mentions = max(1, book.total_mentions // 30)
                    
                    stat = BookDailyStat(
                        book_id=book.id,
                        date=date,
                        daily_views=daily_views,
                        daily_mentions=daily_mentions,
                    )
                    db.add(stat)
                    stats_count += 1
            
            db.commit()
            print(f"âœ… {stats_count} ä»¶ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n")
            
        except Exception as e:
            db.rollback()
            print(f"âŒ çµ±è¨ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
        finally:
            db.close()
    
    def run(self, clear_data: bool = True, max_keywords_per_locale: int = 5):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("\n" + "="*60)
        print("ğŸš€ BookTuber ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        print("="*60 + "\n")
        
        start_time = datetime.now()
        
        try:
            # 1. ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            self.create_tables()
            
            # 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
            if clear_data:
                self.clear_existing_data()
            
            # 3. æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿åé›†
            self.collect_data_for_locale('ja', max_keywords=max_keywords_per_locale)
            
            # 4. è‹±èªãƒ‡ãƒ¼ã‚¿åé›†
            self.collect_data_for_locale('en', max_keywords=max_keywords_per_locale)
            
            # 5. çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            self.generate_statistics()
            
            # å®Œäº†
            elapsed = datetime.now() - start_time
            print("\n" + "="*60)
            print(f"ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"â±ï¸  æ‰€è¦æ™‚é–“: {elapsed.total_seconds():.1f}ç§’")
            print("="*60 + "\n")
            
            print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print("1. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’èµ·å‹•: cd backend && uvicorn app.main:app --reload")
            print("2. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚’èµ·å‹•: cd frontend && npm run dev")
            print("3. ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:3000 ã‚’é–‹ã\n")
            
        except Exception as e:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='BookTuber ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & ãƒ‡ãƒ¼ã‚¿åé›†'
    )
    parser.add_argument(
        '--no-clear',
        action='store_true',
        help='æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ãªã„ï¼ˆè¿½åŠ ãƒ¢ãƒ¼ãƒ‰ï¼‰'
    )
    parser.add_argument(
        '--keywords',
        type=int,
        default=5,
        help='ãƒ­ã‚±ãƒ¼ãƒ«ã”ã¨ã®æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰'
    )
    
    args = parser.parse_args()
    
    setup = BookTuberSetup()
    setup.run(
        clear_data=not args.no_clear,
        max_keywords_per_locale=args.keywords
    )


if __name__ == '__main__':
    main()

